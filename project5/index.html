<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Project 5 · Fun with Diffusion Models</title>

  <!-- MathJax for equations -->
  <script>
    window.MathJax = { tex: { inlineMath: [["$","$"],["\\(","\\)"]] } };
  </script>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <style>
    :root{
      --bg: #f7f7f9; --surface:#fff; --ink:#111827; --muted:#6b7280;
      --border:#e5e7eb; --accent:#2563eb; --radius-lg:16px; --radius-sm:10px;
      --shadow:0 8px 30px rgba(0,0,0,.05); --gap:20px; --font: ui-sans-serif, system    <h6 class="subhead">Illusion 1: Castle / Lighthouse</h6>
    <p class="desc"><strong>Prompts:</strong> "a castle tower drawn in thin black line art" (upright) / "a lighthouse drawn in thin black line art" (flipped)</p>
    <div class="grid cols-2">
      <figure class="tile"><img src="./media/parta/part1/1_8_castle_upright.png" alt="Castle Tower Upright"><figcaption class="cap">Castle (Upright)</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_8_lighthouse_flipped.png" alt="Lighthouse Flipped"><figcaption class="cap">Lighthouse (Flipped)</figcaption></figure>
    </div>
    
    <h6 class="subhead">Illusion 2: Forest / Mountain Lake</h6>
    <p class="desc"><strong>Prompts:</strong> "a symmetric ink drawing of a forest with mirrored trees" (upright) / "an upside-down ink drawing of mountains reflected in water" (flipped)</p>
    <div class="grid cols-2">
      <figure class="tile"><img src="./media/parta/part1/1_8_forest_upright.png" alt="Forest Upright"><figcaption class="cap">Forest (Upright)</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_8_mountain_flipped.png" alt="Mountain Lake Flipped"><figcaption class="cap">Mountain (Flipped)</figcaption></figure>
    </div>

    <h4 class="subhead">1.9 Hybrid Images</h4>ystem, "Segoe UI", Roboto, Helvetica, Arial, "Apple Color Emoji", "Segoe UI Emoji";
    }
    *{box-sizing:border-box}
    html,body{height:100%}
    body{margin:0; background:var(--bg); color:var(--ink); font-family:var(--font); -webkit-font-smoothing:antialiased; line-height:1.6}
    a{color:var(--accent); text-decoration:none}
    a:hover{text-decoration:underline}
    .container{max-width:1100px; margin:32px auto 64px; padding:0 20px}

    .back{display:inline-block; padding:8px 12px; border:1px solid var(--border); border-radius:10px; background:#fff; color:var(--ink); text-decoration:none; box-shadow:var(--shadow)}

    header{margin:24px 0 10px}
    h1{margin:8px 0 6px; font-size:clamp(28px,4.5vw,40px); font-weight:700}
    .meta{margin-top:6px; color:var(--muted); font-weight:600; font-size:14px}

    .card{background:var(--surface); border:1px solid var(--border); border-radius:var(--radius-lg); box-shadow:var(--shadow); padding:24px; margin-top:20px}
    .card h2{margin:0 0 12px; font-size:clamp(18px,2.5vw,24px)}

    .desc,.explanation{margin:8px 0}
    .explanation{color:var(--ink)}

    .tile{background:#fafafb; border:1px solid var(--border); border-radius:var(--radius-sm); padding:12px}

    .grid{display:grid; grid-template-columns:repeat(3,1fr); gap:var(--gap); margin-top:18px}
    .grid.cols-2{grid-template-columns:repeat(2,minmax(340px,1fr)); gap:28px}
    .grid.cols-3{grid-template-columns:repeat(3, minmax(220px,1fr)); gap:18px}
    .grid.cols-4{grid-template-columns:repeat(4, minmax(140px,1fr)); gap:12px}

    .tile img{display:block; width:100%; height:auto; border-radius:8px}
    .cap{text-align:center; color:var(--muted); font-size:14px; margin-top:8px; line-height:1.4}
    .cap small{display:block; margin-top:4px; color:#4b5563}
    .cap code{font-family:ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace; font-size:12.5px; background:#f3f4f6; padding:2px 6px; border-radius:6px}

    pre{background:#0b1220; color:#e5e7eb; padding:16px; border-radius:12px; overflow:auto; border:1px solid #0f172a; box-shadow:inset 0 0 0 1px rgba(255,255,255,.02)}
    code{font-family:ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace; font-size:13px}

    .badge{display:inline-block; font-size:12px; color:#374151; background:#EEF2FF; border:1px solid #E0E7FF; padding:3px 8px; border-radius:999px; margin-right:8px}

    @media (max-width:900px){ .grid{grid-template-columns:repeat(2,1fr)} .grid.cols-2{grid-template-columns:1fr; gap:20px} }
    @media (max-width:600px){ .grid{grid-template-columns:1fr} }

    .contact{text-align:center; margin-top:40px; padding:24px; border-top:1px solid var(--border); color:var(--muted); font-size:14px}
    .contact a{display:inline-block; margin-top:8px; padding:10px 16px; background:var(--accent); color:#fff; border-radius:8px; text-decoration:none; font-weight:600; transition:background .15s ease}
    .contact a:hover{background:#1e4fcf}
    
    .subhead{margin:14px 0 6px; font-weight:700; font-size:16px; color:#1f2937}
  </style>
</head>
<body>
<div class="container">
  <a class="back" href="../index.html">← Back to Home</a>

  <header>
    <h1>Project 5: Fun with Diffusion Models</h1>
    <div class="meta">Dec 2025 · CS180/280A</div>
  </header>

  <section class="card">
    <h2>Overview</h2>
    <p>
      This project explores diffusion models for image generation and manipulation. The first part demonstrates the power of pretrained diffusion models. I implement iterative denoising using DDPM sampling, generate images from pure noise with classifier-free guidance (CFG), and create various image edits including image-to-image translation, inpainting, visual anagrams, and hybrid images. The second part builds diffusion models from scratch using flow matching. Starting with a single-step denoiser on MNIST, I implement a time-conditioned UNet that learns to predict flow from noise to clean data. Finally, I extend the model with class conditioning and classifier-free guidance for controlled generation of specific digits.
    </p>
  </section>

  <!-- Part A -->
  <section class="card" id="part-a">
    <h2>Part A: The Power of Diffusion Models!</h2>
    
    <!-- Part 0 -->
    <h3 class="subhead">Part 0: Setup</h3>
    <p class="desc">I generated embeddings for a variety of interesting text prompts. For this part, I selected three diverse prompts to generate images with different <code>num_inference_steps</code> values to observe how the number of denoising steps affects output quality.</p>
    
    <p class="explanation"><strong>Random Seed:</strong> 180 (used consistently throughout all subsequent parts)</p>
    
    <p class="explanation"><strong>All Prompts Tested:</strong></p>
    <ul>
      <li>a high quality picture</li>
      <li>a watercolor painting of a fox reading under a lantern at night</li>
      <li>a dreamy landscape of floating islands connected by waterfalls</li>
      <li>an ancient library hidden inside a giant tree, fantasy illustration</li>
      <li>a magical koi fish swimming through a sky of clouds, pastel colors</li>
      <li>a fluffy white cat in soft pastel style</li>
      <li>a detailed lion face with sharp textured fur, photorealistic</li>
      <li>a serene beach at sunrise, minimalistic pastel colors</li>
      <li>a rocky coastline with crashing waves, hyper-detailed realism</li>
      <li>a symmetric ink drawing of a forest with mirrored trees</li>
      <li>an upside-down ink drawing of mountains reflected in water</li>
      <li>a geometric pattern forming the shape of a cat</li>
      <li>a geometric pattern forming the shape of a fox</li>
      <li>a castle tower drawn in thin black line art</li>
      <li>a lighthouse drawn in thin black line art</li>
      <li>a smooth stone sculpture of a human skull, minimalistic lighting</li>
      <li>a hyper-detailed roaring waterfall with sparkling mist and rock textures</li>
      <li>a soft pastel painting of a mountain silhouette at dusk</li>
      <li>a neon-lit futuristic cityscape with intricate glowing details</li>
      <li>a simple charcoal sketch of an owl face, large shapes emphasized</li>
      <li>a dense, high-detail forest with leaves, moss, and textured bark</li>
      <li>a clean watercolor illustration of the full moon</li>
      <li>complex brass clockwork machinery full of tiny gears and engravings</li>
      <li>rocket ship</li>
    </ul>
    
    <p class="explanation"><strong>Selected Prompts for Part 0:</strong></p>
    <ul>
      <li>a dreamy landscape of floating islands connected by waterfalls</li>
      <li>a magical koi fish swimming through a sky of clouds, pastel colors</li>
      <li>an ancient library hidden inside a giant tree, fantasy illustration</li>
    </ul>
    
    <p class="explanation">Each prompt was tested with three different inference step values (20, 50, and 100 steps) to compare generation quality. With fewer steps, the model produces rougher, less detailed images, while higher step counts yield smoother, more refined results with better adherence to the text prompt. The 3×3 grid below shows how each prompt evolves across different inference steps.</p>
    
    <div class="grid cols-3">
      <figure class="tile"><img src="./media/parta/part0/island_20.png" alt="Islands 20 steps"><figcaption class="cap">Floating islands<br><small>20 steps</small></figcaption></figure>
      <figure class="tile"><img src="./media/parta/part0/island_50.png" alt="Islands 50 steps"><figcaption class="cap">Floating islands<br><small>50 steps</small></figcaption></figure>
      <figure class="tile"><img src="./media/parta/part0/island_100.png" alt="Islands 100 steps"><figcaption class="cap">Floating islands<br><small>100 steps</small></figcaption></figure>

      <figure class="tile"><img src="./media/parta/part0/koifish_20.png" alt="Koi 20 steps"><figcaption class="cap">Magical koi fish<br><small>20 steps</small></figcaption></figure>
      <figure class="tile"><img src="./media/parta/part0/koifish_50.png" alt="Koi 50 steps"><figcaption class="cap">Magical koi fish<br><small>50 steps</small></figcaption></figure>
      <figure class="tile"><img src="./media/parta/part0/koifish_100.png" alt="Koi 100 steps"><figcaption class="cap">Magical koi fish<br><small>100 steps</small></figcaption></figure>

      <figure class="tile"><img src="./media/parta/part0/tree_20.png" alt="Library 20 steps"><figcaption class="cap">Ancient library tree<br><small>20 steps</small></figcaption></figure>
      <figure class="tile"><img src="./media/parta/part0/tree_50.png" alt="Library 50 steps"><figcaption class="cap">Ancient library tree<br><small>50 steps</small></figcaption></figure>
      <figure class="tile"><img src="./media/parta/part0/tree_100.png" alt="Library 100 steps"><figcaption class="cap">Ancient library tree<br><small>100 steps</small></figcaption></figure>
    </div>
    
    <!-- Part 1 -->
    <h3 class="subhead">Part 1: Sampling Loops</h3>
    
    <h4 class="subhead">1.1 Implementing the Forward Process</h4>
    
    <p class="desc">The forward process simulates how diffusion models progressively corrupt clean images by adding Gaussian noise over time. This is a fundamental step that allows the model to later learn how to reverse the process (denoising).</p>
    
    <p class="explanation">Given a clean image $x_0$, we compute the noisy image at timestep $t$ using:</p>
    
    <p class="explanation">$$x_t = \sqrt{\bar{\alpha}_t} \, x_0 + \sqrt{1 - \bar{\alpha}_t} \, \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)$$</p>
    
    <p class="explanation"><strong>Implementation:</strong> The process involves three key steps:</p>
    <ul class="explanation">
      <li><strong>Extract noise schedule parameter:</strong> Retrieve $\bar{\alpha}_t$ from the pre-computed <code>alphas_cumprod</code> array for timestep $t$</li>
      <li><strong>Generate random noise:</strong> Sample $\epsilon \sim \mathcal{N}(0, I)$ using <code>torch.randn_like(im)</code></li>
      <li><strong>Weighted combination:</strong> Scale the original image by $\sqrt{\bar{\alpha}_t}$ and the noise by $\sqrt{1 - \bar{\alpha}_t}$, then combine them</li>
    </ul>
    
    <p class="explanation">The scaling factors ensure that as $t$ increases (more noise), $\bar{\alpha}_t$ decreases toward 0, reducing the original image's contribution while increasing the noise contribution. At $t=0$ (clean), $\bar{\alpha}_t \approx 1$ preserves the original image; at high $t$, the image becomes pure noise.</p>
    
    <p class="explanation">Below, I show the Campanile image at noise levels $t = 250$, $t = 500$, and $t = 750$:</p>
    
    <div class="grid cols-4">
      <figure class="tile"><img src="./media/parta/part1/campanile_original.png" alt="Original Campanile"><figcaption class="cap">Original<br><small>t = 0</small></figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_1_campanile_250.png" alt="Campanile t=250"><figcaption class="cap">Noisy Campanile<br><small>t = 250</small></figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_1_campanile_500.png" alt="Campanile t=500"><figcaption class="cap">Noisy Campanile<br><small>t = 500</small></figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_1_campanile_750.png" alt="Campanile t=750"><figcaption class="cap">Noisy Campanile<br><small>t = 750</small></figcaption></figure>
    </div>

    <h4 class="subhead">1.2 Classical Denoising</h4>
    <p class="desc">Classical denoising methods like Gaussian blur filtering struggle to remove the noise added by the diffusion forward process. Below, I apply Gaussian blur to the noisy Campanile images from Part 1.1 at timesteps $t = 250, 500, 750$. The results show that simple filtering cannot effectively recover the original image structure, especially at higher noise levels.</p>
    
    <div class="grid cols-3">
      <figure class="tile"><img src="./media/parta/part1/1_1_campanile_250.png" alt="Noisy t=250"><figcaption class="cap">Noisy<br><small>t = 250</small></figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_1_campanile_500.png" alt="Noisy t=500"><figcaption class="cap">Noisy<br><small>t = 500</small></figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_1_campanile_750.png" alt="Noisy t=750"><figcaption class="cap">Noisy<br><small>t = 750</small></figcaption></figure>
      
      <figure class="tile"><img src="./media/parta/part1/1_2_campanile_250.png" alt="Gaussian denoised t=250"><figcaption class="cap">Gaussian Denoised<br><small>t = 250</small></figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_2_campanile_500.png" alt="Gaussian denoised t=500"><figcaption class="cap">Gaussian Denoised<br><small>t = 500</small></figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_2_campanile_750.png" alt="Gaussian denoised t=750"><figcaption class="cap">Gaussian Denoised<br><small>t = 750</small></figcaption></figure>
    </div>

    <h4 class="subhead">1.3 One-Step Denoising</h4>
    <p class="desc">We can leverage a pretrained diffusion model UNet (<code>stage_1.unet</code>) to denoise images. The UNet estimates the noise $\epsilon$ in a noisy image conditioned on timestep $t$ and a text prompt embedding. By removing the estimated noise, we recover an approximation of the original image in a single step. Below shows results for $t = 250, 500, 750$ using the prompt "a high quality picture":</p>
    
    <div class="grid cols-3">
      <figure class="tile"><img src="./media/parta/part1/campanile_original.png" alt="Original"><figcaption class="cap">Original<br><small>t = 250</small></figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/campanile_original.png" alt="Original"><figcaption class="cap">Original<br><small>t = 500</small></figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/campanile_original.png" alt="Original"><figcaption class="cap">Original<br><small>t = 750</small></figcaption></figure>
      
      <figure class="tile"><img src="./media/parta/part1/1_1_campanile_250.png" alt="Noisy t=250"><figcaption class="cap">Noisy<br><small>t = 250</small></figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_1_campanile_500.png" alt="Noisy t=500"><figcaption class="cap">Noisy<br><small>t = 500</small></figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_1_campanile_750.png" alt="Noisy t=750"><figcaption class="cap">Noisy<br><small>t = 750</small></figcaption></figure>
      
      <figure class="tile"><img src="./media/parta/part1/1_3_campanile_250.png" alt="One-step denoised t=250"><figcaption class="cap">One-Step Denoised<br><small>t = 250</small></figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_3_campanile_500.png" alt="One-step denoised t=500"><figcaption class="cap">One-Step Denoised<br><small>t = 500</small></figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_3_campanile_750.png" alt="One-step denoised t=750"><figcaption class="cap">One-Step Denoised<br><small>t = 750</small></figcaption></figure>
    </div>

    <h4 class="subhead">1.4 Iterative Denoising</h4>
    
    <p class="desc">While one-step denoising works reasonably well, diffusion models are designed to denoise iteratively. Instead of running all 1000 timesteps (slow and expensive), we can skip steps using a strided schedule. This allows us to progressively remove noise in fewer steps while maintaining high quality.</p>
    
    <p class="explanation"><strong>Implementation:</strong> The process involves three key components:</p>
    <ul class="explanation">
      <li><strong>Create strided timesteps:</strong> Generate a list from $t=990$ to $t=0$ with stride 30</li>
      <li><strong>Estimate clean image:</strong> At each step $t$, use the UNet to predict noise $\epsilon_t$, then compute $\hat{x}_0 = \frac{x_t - \sqrt{1-\bar{\alpha}_t} \epsilon_t}{\sqrt{\bar{\alpha}_t}}$</li>
      <li><strong>DDPM update step:</strong> Move from $x_t$ to $x_{t'}$ (less noisy) using a weighted combination of the estimated clean image and current noisy image</li>
    </ul>
    
    <p class="explanation">The DDPM update formula interpolates between signal and noise:</p>
    
    <p class="explanation">$$x_{t'} = \frac{\sqrt{\bar{\alpha}_{t'}} \cdot \beta_t}{1 - \bar{\alpha}_t} \hat{x}_0 + \frac{\sqrt{\alpha_t}(1 - \bar{\alpha}_{t'})}{1 - \bar{\alpha}_t} x_t + v_\sigma$$</p>
    
    <p class="explanation">where $\alpha_t = \bar{\alpha}_t / \bar{\alpha}_{t'}$, $\beta_t = 1 - \alpha_t$, and $v_\sigma$ is learned variance. This gradually shifts from noisy to clean while maintaining image structure.</p>
    
    <!-- <p class="explanation"><strong>Setup:</strong> Create strided timesteps starting at 990 with stride 30:</p>
    <pre><code>strided_timesteps = list(range(990, -1, -30))
stage_1.scheduler.set_timesteps(timesteps=strided_timesteps)</code></pre> -->
    
    <!-- <p class="explanation"><strong>Iterative Denoising Function:</strong> On the $i$-th step, we move from $t$ to $t'$ (less noisy) using the DDPM update rule:</p>
    <pre><code>def iterative_denoise(im_noisy, i_start, prompt_embeds, timesteps, display=True):
  image = im_noisy

  with torch.no_grad():
    for i in range(i_start, len(timesteps) - 1):
      # Get timesteps
      t = timesteps[i]
      prev_t = timesteps[i+1]

      # get `alpha_cumprod` and `alpha_cumprod_prev` for timestep t from `alphas_cumprod`
      # compute `alpha`
      # compute `beta`
      alpha_bar_t = alphas_cumprod[t].to(device)
      alpha_bar_prev = alphas_cumprod[prev_t].to(device)

      alpha = alpha_bar_t / alpha_bar_prev
      beta = 1.0 - alpha

      image = image.to(device).half()
      prompt_embeds = prompt_embeds.to(device).half()

      # Get noise estimate
      model_output = stage_1.unet(
          image,
          t,
          encoder_hidden_states=prompt_embeds,
          return_dict=False
      )[0]

      # Split estimate into noise and variance estimate
      noise_est, predicted_variance = torch.split(model_output, image.shape[1], dim=1)

      # compute `pred_prev_image` (x_{t'}), the DDPM estimate for the image at the
      # next timestep, which is slightly less noisy. Use the equation 3.
      # This is the core of DDPM
      sqrt_alpha_bar_t = torch.sqrt(alpha_bar_t)
      sqrt_one_minus_alpha_bar_t = torch.sqrt(1.0 - alpha_bar_t)

      x0_est = (image - sqrt_one_minus_alpha_bar_t.view(1, 1, 1, 1) * noise_est) / sqrt_alpha_bar_t.view(1, 1, 1, 1)

      term_1 = (torch.sqrt(alpha_bar_prev) * beta) / (1.0 - alpha_bar_t)
      term_2 = (torch.sqrt(alpha) * (1.0 - alpha_bar_prev)) / (1.0 - alpha_bar_t)

      pred_prev_image = term_1.view(1, 1, 1, 1) * x0_est + term_2.view(1, 1, 1, 1) * image
      pred_prev_image = add_variance(predicted_variance, t, pred_prev_image)

      image = pred_prev_image

      if display and ((i - i_start) % 5 == 0):
        img = (image[0].detach().cpu().permute(1, 2, 0) / 2. + 0.5).numpy()
        print(f"{timesteps[i]}")
        media.show_image(img)

    clean = image.cpu().detach().numpy()

  return clean</code></pre> -->
    
    <p class="explanation"><strong>Results:</strong> Below shows the noisy Campanile every 5th loop of denoising, gradually becoming less noisy. The final predicted clean image using iterative denoising is compared against the one-step denoising result from Part 1.3 (which looks much worse) and the Gaussian blur result from Part 1.2:</p>
    
    <div class="grid cols-3">
      <figure class="tile"><img src="./media/parta/part1/1_4_campanile_90.png" alt="t=90"><figcaption class="cap">Iterative<br><small>t = 90</small></figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_4_campanile_240.png" alt="t=240"><figcaption class="cap">Iterative<br><small>t = 240</small></figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_4_campanile_390.png" alt="t=390"><figcaption class="cap">Iterative<br><small>t = 390</small></figcaption></figure>
      
      <figure class="tile"><img src="./media/parta/part1/1_4_campanile_540.png" alt="t=540"><figcaption class="cap">Iterative<br><small>t = 540</small></figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_4_campanile_690.png" alt="t=690"><figcaption class="cap">Iterative<br><small>t = 690</small></figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/campanile_original.png" alt="Original"><figcaption class="cap">Original</figcaption></figure>
      
      <figure class="tile"><img src="./media/parta/part1/1_4_iterative_denoising_final.png" alt="Final"><figcaption class="cap">Iterative Final</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_4_one_step_denoising_final.png" alt="One-step"><figcaption class="cap">One-Step</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_4_gaussian_blur_final.png" alt="Gaussian"><figcaption class="cap">Gaussian Blur</figcaption></figure>
    </div>

    <h4 class="subhead">1.5 Diffusion Model Sampling</h4>
    <p class="desc">Beyond denoising existing images, we can generate images from scratch by denoising pure random noise. By setting <code>i_start = 0</code> and passing random noise as <code>im_noisy</code>, the iterative denoising function effectively generates new images. Below shows 5 sampled results using the prompt "a high quality picture":</p>
    
    <div class="grid" style="grid-template-columns:repeat(5, 1fr); gap:8px;">
      <figure class="tile"><img src="./media/parta/part1/1_5_sample1.png" alt="Sample 1"><figcaption class="cap">Sample 1</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_5_sample2.png" alt="Sample 2"><figcaption class="cap">Sample 2</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_5_sample3.png" alt="Sample 3"><figcaption class="cap">Sample 3</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_5_sample4.png" alt="Sample 4"><figcaption class="cap">Sample 4</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_5_sample5.png" alt="Sample 5"><figcaption class="cap">Sample 5</figcaption></figure>
    </div>

    <h4 class="subhead">1.6 Classifier-Free Guidance (CFG)</h4>
    <p class="desc">The images from Part 1.5 lack quality and coherence. Classifier-Free Guidance (CFG) dramatically improves image quality (at the expense of diversity) by computing both conditional ($\epsilon_{\text{cond}}$) and unconditional ($\epsilon_{\text{uncond}}$) noise estimates. The unconditional estimate uses an empty prompt embedding (the model was trained to predict unconditional noise for empty prompts). The CFG noise estimate combines these:</p>
    
    <p class="explanation">$$\epsilon = \epsilon_{\text{uncond}} + \gamma \, (\epsilon_{\text{cond}} - \epsilon_{\text{uncond}})$$</p>
    
    <p class="explanation">where $\gamma$ controls CFG strength. For $\gamma = 1$, we get the conditional estimate; for $\gamma > 1$, we extrapolate beyond it for much higher quality.</p>
    
    <!-- <p class="explanation"><strong>Setup:</strong></p>
    <pre><code># The conditional prompt embedding
prompt_embeds = prompt_embeds_dict['a high quality picture']

# The unconditional prompt embedding
uncond_prompt_embeds = prompt_embeds_dict['']</code></pre> -->
    
    <!-- <p class="explanation"><strong>Iterative Denoising with CFG:</strong></p>
    <pre><code>def iterative_denoise_cfg(im_noisy, i_start, prompt_embeds, uncond_prompt_embeds, timesteps, scale=7, display=True):
  image = im_noisy

  with torch.no_grad():
    for i in range(i_start, len(timesteps) - 1):
      # Get timesteps
      t = timesteps[i]
      prev_t = timesteps[i+1]

      # Get `alpha_cumprod`, `alpha_cumprod_prev`, `alpha`, `beta`
      alpha_bar_t = alphas_cumprod[t].to(device)
      alpha_bar_prev = alphas_cumprod[prev_t].to(device)

      alpha = alpha_bar_t / alpha_bar_prev
      beta = 1.0 - alpha

      image = image.to(device).half()
      prompt_embeds = prompt_embeds.to(device).half()
      uncond_prompt_embeds = uncond_prompt_embeds.to(device).half()

      # Get cond noise estimate
      model_output = stage_1.unet(
          image,
          t,
          encoder_hidden_states=prompt_embeds,
          return_dict=False
      )[0]

      # Get uncond noise estimate
      uncond_model_output = stage_1.unet(
          image,
          t,
          encoder_hidden_states=uncond_prompt_embeds,
          return_dict=False
      )[0]

      # Split estimate into noise and variance estimate
      noise_est, predicted_variance = torch.split(model_output, image.shape[1], dim=1)
      uncond_noise_est, _ = torch.split(uncond_model_output, image.shape[1], dim=1)

      # Compute the CFG noise estimate based on equation 4
      noise_est = uncond_noise_est + scale * (noise_est - uncond_noise_est)

      # Get `pred_prev_image`, the next less noisy image.
      sqrt_alpha_bar_t = torch.sqrt(alpha_bar_t)
      sqrt_one_minus_alpha_bar_t = torch.sqrt(1.0 - alpha_bar_t)

      x0_est = (image - sqrt_one_minus_alpha_bar_t.view(1,1,1,1) * noise_est) / sqrt_alpha_bar_t.view(1,1,1,1)

      term_1 = (torch.sqrt(alpha_bar_prev) * beta) / (1.0 - alpha_bar_t)
      term_2 = (torch.sqrt(alpha) * (1.0 - alpha_bar_prev)) / (1.0 - alpha_bar_t)

      pred_prev_image = term_1.view(1,1,1,1) * x0_est + term_2.view(1,1,1,1) * image
      pred_prev_image = add_variance(predicted_variance, t, pred_prev_image)

      image = pred_prev_image

    clean = image.cpu().detach().numpy()

  return clean</code></pre> -->
    
    <p class="explanation"><strong>Results:</strong> Below shows 5 images generated with CFG scale $\gamma = 7$ using the prompt "a high quality picture". Compared to Part 1.5, the images show improved quality.</p>
    <div class="grid" style="grid-template-columns:repeat(5, 1fr); gap:8px;">
      <figure class="tile"><img src="./media/parta/part1/1_6_sample1.png" alt="CFG Sample 1"><figcaption class="cap">Sample 1</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_6_sample2.png" alt="CFG Sample 2"><figcaption class="cap">Sample 2</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_6_sample3.png" alt="CFG Sample 3"><figcaption class="cap">Sample 3</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_6_sample4.png" alt="CFG Sample 4"><figcaption class="cap">Sample 4</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_6_sample5.png" alt="CFG Sample 5"><figcaption class="cap">Sample 5</figcaption></figure>
    </div>

    <h4 class="subhead">1.7 Image-to-image Translation</h4>
    <p class="desc">By adding noise to a real image and then denoising it, we can edit existing images. More noise produces larger edits, as the diffusion model must be more creative to denoise—effectively forcing the noisy image back onto the natural image manifold. This follows the SDEdit algorithm. Below, I noise the Campanile and two test images, then denoise starting at different noise levels (indexed by starting step $i_{\text{start}} = [1, 3, 5, 7, 10, 20]$) using the prompt "a high quality photo". Lower indices = more noise = larger edits:</p>
    
    <h5 class="subhead">Campanile Edits</h5>
    <div class="grid cols-4">
      <figure class="tile"><img src="./media/parta/part1/1_7_campanile_1.png" alt="i=1"><figcaption class="cap">i = 1</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_campanile_3.png" alt="i=3"><figcaption class="cap">i = 3</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_campanile_5.png" alt="i=5"><figcaption class="cap">i = 5</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_campanile_7.png" alt="i=7"><figcaption class="cap">i = 7</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_campanile_10.png" alt="i=10"><figcaption class="cap">i = 10</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_campanile_20.png" alt="i=20"><figcaption class="cap">i = 20</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/campanile_original.png" alt="Original"><figcaption class="cap">Original</figcaption></figure>
    </div>
    
    <h5 class="subhead">Test Image 1: Dessert</h5>
    <div class="grid cols-4">
      <figure class="tile"><img src="./media/parta/part1/1_7_dessert_1.png" alt="i=1"><figcaption class="cap">i = 1</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_dessert_3.png" alt="i=3"><figcaption class="cap">i = 3</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_dessert_5.png" alt="i=5"><figcaption class="cap">i = 5</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_dessert_7.png" alt="i=7"><figcaption class="cap">i = 7</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_dessert_10.png" alt="i=10"><figcaption class="cap">i = 10</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_dessert_20.png" alt="i=20"><figcaption class="cap">i = 20</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_dessert_original.png" alt="Original"><figcaption class="cap">Original</figcaption></figure>
    </div>
    
    <h5 class="subhead">Test Image 2: Lake</h5>
    <div class="grid cols-4">
      <figure class="tile"><img src="./media/parta/part1/1_7_lake_1.png" alt="i=1"><figcaption class="cap">i = 1</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_lake_3.png" alt="i=3"><figcaption class="cap">i = 3</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_lake_5.png" alt="i=5"><figcaption class="cap">i = 5</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_lake_7.png" alt="i=7"><figcaption class="cap">i = 7</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_lake_10.png" alt="i=10"><figcaption class="cap">i = 10</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_lake_20.png" alt="i=20"><figcaption class="cap">i = 20</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_lake_original.png" alt="Original"><figcaption class="cap">Original</figcaption></figure>
    </div>

    <h5 class="subhead">1.7.1 Editing Hand-Drawn and Web Images</h5>
    <p class="desc">The image-to-image procedure works especially well with non-realistic inputs like drawings. By projecting these onto the natural image manifold, we can create photorealistic versions. Below shows one web image and two hand-drawn images edited at noise levels $i_{\text{start}} = [1, 3, 5, 7, 10, 15, 20]$:</p>
    

    <h6 class="subhead">Web Image: Mountain</h6>
    <div class="grid cols-4">
      <figure class="tile"><img src="./media/parta/part1/1_7_1_mountain_1.png" alt="i=1"><figcaption class="cap">i = 1</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_1_mountain_3.png" alt="i=3"><figcaption class="cap">i = 3</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_1_mountain_5.png" alt="i=5"><figcaption class="cap">i = 5</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_1_mountain_7.png" alt="i=7"><figcaption class="cap">i = 7</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_1_mountain_10.png" alt="i=10"><figcaption class="cap">i = 10</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_1_mountain_15.png" alt="i=15"><figcaption class="cap">i = 15</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_1_mountain_20.png" alt="i=20"><figcaption class="cap">i = 20</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_1_mountain_original.png" alt="Original"><figcaption class="cap">Original</figcaption></figure>
    </div>

    <h6 class="subhead">Hand-Drawn 1: Boat</h6>
    <div class="grid cols-4">
      <figure class="tile"><img src="./media/parta/part1/1_7_1_boat_1.png" alt="i=1"><figcaption class="cap">i = 1</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_1_boat_3.png" alt="i=3"><figcaption class="cap">i = 3</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_1_boat_5.png" alt="i=5"><figcaption class="cap">i = 5</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_1_boat_7.png" alt="i=7"><figcaption class="cap">i = 7</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_1_boat_10.png" alt="i=10"><figcaption class="cap">i = 10</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_1_boat_15.png" alt="i=15"><figcaption class="cap">i = 15</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_1_boat_20.png" alt="i=20"><figcaption class="cap">i = 20</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_1_boat_original.png" alt="Original"><figcaption class="cap">Original</figcaption></figure>
    </div>
    
    <h6 class="subhead">Hand-Drawn 2: Flower</h6>
    <div class="grid cols-4">
      <figure class="tile"><img src="./media/parta/part1/1_7_1_flower_1.png" alt="i=1"><figcaption class="cap">i = 1</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_1_flower_3.png" alt="i=3"><figcaption class="cap">i = 3</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_1_flower_5.png" alt="i=5"><figcaption class="cap">i = 5</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_1_flower_7.png" alt="i=7"><figcaption class="cap">i = 7</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_1_flower_10.png" alt="i=10"><figcaption class="cap">i = 10</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_1_flower_15.png" alt="i=15"><figcaption class="cap">i = 15</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_1_flower_20.png" alt="i=20"><figcaption class="cap">i = 20</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_1_flower_original.png" alt="Original"><figcaption class="cap">Original</figcaption></figure>
    </div>

    <h5 class="subhead">1.7.2 Inpainting</h5>
    
    <p class="desc">Inpainting allows us to edit specific regions of an image while preserving the rest. Following the RePaint algorithm, we can generate new content in masked areas while keeping unmasked areas identical to the original image throughout the diffusion process.</p>
    
    <p class="explanation"><strong>Implementation:</strong> The key insight is to combine iterative denoising with CFG, but add a projection step at each iteration:</p>
    <ul class="explanation">
      <li><strong>Start with random noise:</strong> Initialize the entire image as noise (not just the masked region)</li>
      <li><strong>Projection at each step:</strong> After computing $x_t$, replace unmasked pixels with the correctly-noised original</li>
      <li><strong>CFG for quality:</strong> Use classifier-free guidance to generate high-quality content in the masked region</li>
    </ul>
    
    <p class="explanation">The projection formula enforces consistency with the original:</p>
    
    <p class="equation">$$x_t \leftarrow m x_t + (1 - m) \text{forward}(x_{\text{orig}}, t)$$</p>
    
    <p class="explanation">where $m$ is the binary mask (1 = edit region, 0 = preserve region). This ensures that at each timestep $t$, the unmasked area contains the original image with the exact noise level expected at that timestep, creating a seamless blend between generated and original content.</p>
    
    <!-- <h6 class="subhead">Implementation</h6>
    <pre class="code"><code>def inpaint(original_image, mask, prompt_embeds, uncond_prompt_embeds, timesteps, scale=7, display=True):
  image = torch.randn_like(original_image).to(device).half()

  # use your previous `iterative_denoise_cfg` function and make the appropriate changes
  original_image = original_image.to(device).half()
  mask = mask.to(device).to(image.dtype)

  prompt_embeds = prompt_embeds.to(device).half()
  uncond_prompt_embeds = uncond_prompt_embeds.to(device).half()

  with torch.no_grad():
    for i in range(len(timesteps) - 1):
      t = timesteps[i]
      prev_t = timesteps[i + 1]

      orig_noisy = forward(original_image, t).to(device).half()
      image = mask * image + (1.0 - mask) * orig_noisy

      alpha_bar_t = alphas_cumprod[t].to(device)
      alpha_bar_prev = alphas_cumprod[prev_t].to(device)

      alpha = alpha_bar_t / alpha_bar_prev
      beta = 1.0 - alpha

      image = image.to(device).half()
      prompt_embeds = prompt_embeds.to(device).half()
      uncond_prompt_embeds = uncond_prompt_embeds.to(device).half()

      model_output = stage_1.unet(
          image,
          t,
          encoder_hidden_states=prompt_embeds,
          return_dict=False
      )[0]

      uncond_model_output = stage_1.unet(
          image,
          t,
          encoder_hidden_states=uncond_prompt_embeds,
          return_dict=False
      )[0]

      noise_est, predicted_variance = torch.split(model_output, image.shape[1], dim=1)
      uncond_noise_est, _ = torch.split(uncond_model_output, image.shape[1], dim=1)

      noise_est = uncond_noise_est + scale * (noise_est - uncond_noise_est)

      sqrt_alpha_bar_t = torch.sqrt(alpha_bar_t)
      sqrt_one_minus_alpha_bar_t = torch.sqrt(1.0 - alpha_bar_t)

      x0_est = (image - sqrt_one_minus_alpha_bar_t.view(1, 1, 1, 1) * noise_est) / sqrt_alpha_bar_t.view(1, 1, 1, 1)

      term_1 = (torch.sqrt(alpha_bar_prev) * beta) / (1.0 - alpha_bar_t)
      term_2 = (torch.sqrt(alpha) * (1.0 - alpha_bar_prev)) / (1.0 - alpha_bar_t)

      pred_prev_image = term_1.view(1, 1, 1, 1) * x0_est + term_2.view(1, 1, 1, 1) * image
      pred_prev_image = add_variance(predicted_variance, t, pred_prev_image)

      image = pred_prev_image

      if display and (i % 5 == 0):
        img = (image[0].detach().cpu().permute(1, 2, 0) / 2. + 0.5).numpy()
        print(f"{timesteps[i]}")
        media.show_image(img)

  clean = image.cpu().detach().numpy()
  return clean</code></pre> -->
    
    <h6 class="subhead">Campanile Inpainting</h6>
    <div class="grid cols-4">
      <figure class="tile"><img src="./media/parta/part1/campanile_original.png" alt="Original"><figcaption class="cap">Original</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_2_campanile_mask.png" alt="Mask"><figcaption class="cap">Mask</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_2_campanile_hole_to_fill.png" alt="Hole to Fill"><figcaption class="cap">Hole to Fill</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_2_campanile_inpainted.png" alt="Inpainted"><figcaption class="cap">Inpainted</figcaption></figure>
    </div>
    
    <h6 class="subhead">Lilypad Inpainting</h6>
    <div class="grid cols-4">
      <figure class="tile"><img src="./media/parta/part1/1_7_2_lilypad_original.png" alt="Original"><figcaption class="cap">Original</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_2_lilypad_mask.png" alt="Mask"><figcaption class="cap">Mask</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_2_lilypad_hole_to_fill.png" alt="Hole to Fill"><figcaption class="cap">Hole to Fill</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_2_lilypad_inpainted.png" alt="Inpainted"><figcaption class="cap">Inpainted</figcaption></figure>
    </div>
    
    <h6 class="subhead">Bridge Inpainting</h6>
    <div class="grid cols-4">
      <figure class="tile"><img src="./media/parta/part1/1_7_2_bridge_original.png" alt="Original"><figcaption class="cap">Original</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_2_bridge_mask.png" alt="Mask"><figcaption class="cap">Mask</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_2_bridge_hole_to_fill.png" alt="Hole to Fill"><figcaption class="cap">Hole to Fill</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_2_bridge_inpainted.png" alt="Inpainted"><figcaption class="cap">Inpainted</figcaption></figure>
    </div>

    <h5 class="subhead">1.7.3 Text-Conditional Image-to-image Translation</h5>
    <p class="desc">We apply the same SDEdit procedure but guide the projection with a text prompt. Instead of using "a high quality picture", we use specific prompts to control the style and content of the denoised result. This combines projection to the natural image manifold with language-based control. Below shows edits at noise levels $i_{\text{start}} = [1, 3, 5, 7, 10, 20]$:</p>
    
    <h6 class="subhead">Campanile with "rocket ship"</h6>
    <div class="grid cols-4">
      <figure class="tile"><img src="./media/parta/part1/1_7_3_campanile_1.png" alt="i=1"><figcaption class="cap">i = 1</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_3_campanile_3.png" alt="i=3"><figcaption class="cap">i = 3</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_3_campanile_5.png" alt="i=5"><figcaption class="cap">i = 5</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_3_campanile_7.png" alt="i=7"><figcaption class="cap">i = 7</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_3_campanile_10.png" alt="i=10"><figcaption class="cap">i = 10</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_3_campanile_20.png" alt="i=20"><figcaption class="cap">i = 20</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/campanile_original.png" alt="Original"><figcaption class="cap">Original</figcaption></figure>
    </div>
    
    <h6 class="subhead">City with "a serene beach at sunrise, minimalistic pastel colors"</h6>
    <div class="grid cols-4">
      <figure class="tile"><img src="./media/parta/part1/1_7_3_city_1.png" alt="i=1"><figcaption class="cap">i = 1</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_3_city_3.png" alt="i=3"><figcaption class="cap">i = 3</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_3_city_5.png" alt="i=5"><figcaption class="cap">i = 5</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_3_city_7.png" alt="i=7"><figcaption class="cap">i = 7</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_3_city_10.png" alt="i=10"><figcaption class="cap">i = 10</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_3_city_20.png" alt="i=20"><figcaption class="cap">i = 20</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_3_city_original.png" alt="Original"><figcaption class="cap">Original</figcaption></figure>
    </div>
    
    <h6 class="subhead">Puppy with "a fluffy white cat in soft pastel style"</h6>
    <div class="grid cols-4">
      <figure class="tile"><img src="./media/parta/part1/1_7_3_puppy_1.png" alt="i=1"><figcaption class="cap">i = 1</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_3_puppy_3.png" alt="i=3"><figcaption class="cap">i = 3</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_3_puppy_5.png" alt="i=5"><figcaption class="cap">i = 5</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_3_puppy_7.png" alt="i=7"><figcaption class="cap">i = 7</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_3_puppy_10.png" alt="i=10"><figcaption class="cap">i = 10</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_3_puppy_20.png" alt="i=20"><figcaption class="cap">i = 20</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_7_3_puppy_original.png" alt="Original"><figcaption class="cap">Original</figcaption></figure>
    </div>

    <h4 class="subhead">1.8 Visual Anagrams</h4>
    
    <p class="desc">Visual anagrams create optical illusions where an image shows different content depending on its orientation. By simultaneously optimizing for two different prompts (one upright, one flipped), we can generate images that satisfy both interpretations.</p>
    
    <p class="explanation"><strong>Implementation:</strong> The key is to compute a composite noise estimate that guides the image toward both prompts:</p>
    <ul class="explanation">
      <li><strong>Denoise upright:</strong> Apply CFG with prompt $p_1$ to get noise estimate $\epsilon_1$ for the normal orientation</li>
      <li><strong>Denoise flipped:</strong> Flip $x_t$ upside down, apply CFG with prompt $p_2$ to get $\epsilon_2$, then flip $\epsilon_2$ back</li>
      <li><strong>Average noise estimates:</strong> Compute $\epsilon = (\epsilon_1 + \epsilon_2) / 2$ to create a compromise that satisfies both prompts</li>
    </ul>
    
    <p class="explanation">The algorithm at each denoising step:</p>
    
    <p class="equation">$$\epsilon_1 = \text{CFG}(\text{UNet}(x_t, t, p_1))$$</p>
    <p class="equation">$$\epsilon_2 = \text{flip}(\text{CFG}(\text{UNet}(\text{flip}(x_t), t, p_2)))$$</p>
    <p class="equation">$$\epsilon = (\epsilon_1 + \epsilon_2) / 2$$</p>
    
    <p class="explanation">By averaging the noise estimates, we force the model to find visual features that work for both orientations. This creates images with dual interpretations—structures that suggest one subject upright but reveal another when flipped.</p>
    
    <!-- <h6 class="subhead">Implementation</h6>
    <pre class="code"><code>def make_flip_illusion(image, i_start, prompt_embeds, uncond_prompt_embeds, timesteps, scale=7, display=True):
    prompt1_embeds, prompt2_embeds = prompt_embeds

    image = image.to(device).half()
    prompt1_embeds = prompt1_embeds.to(device).half()
    prompt2_embeds = prompt2_embeds.to(device).half()
    uncond_prompt_embeds = uncond_prompt_embeds.to(device).half()

    with torch.no_grad():
      for i in range(i_start, len(timesteps) - 1):
        t = timesteps[i]
        prev_t = timesteps[i + 1]

        alpha_bar_t = alphas_cumprod[t].to(device)
        alpha_bar_prev = alphas_cumprod[prev_t].to(device)
        alpha = alpha_bar_t / alpha_bar_prev
        beta = 1.0 - alpha

        image = image.to(device).half()

        model_output_1 = stage_1.unet(
            image,
            t,
            encoder_hidden_states=prompt1_embeds,
            return_dict=False
        )[0]

        uncond_output_1 = stage_1.unet(
            image,
            t,
            encoder_hidden_states=uncond_prompt_embeds,
            return_dict=False
        )[0]

        noise_1, predicted_variance = torch.split(model_output_1, image.shape[1], dim=1)
        uncond_noise_1, _ = torch.split(uncond_output_1, image.shape[1], dim=1)

        eps_1 = uncond_noise_1 + scale * (noise_1 - uncond_noise_1)

        image_flipped = torch.flip(image, dims=[2])

        model_output_2 = stage_1.unet(
            image_flipped,
            t,
            encoder_hidden_states=prompt2_embeds,
            return_dict=False
        )[0]

        uncond_output_2 = stage_1.unet(
            image_flipped,
            t,
            encoder_hidden_states=uncond_prompt_embeds,
            return_dict=False
        )[0]

        noise_2, _ = torch.split(model_output_2, image.shape[1], dim=1)
        uncond_noise_2, _ = torch.split(uncond_output_2, image.shape[1], dim=1)

        eps_2 = uncond_noise_2 + scale * (noise_2 - uncond_noise_2)
        eps_2 = torch.flip(eps_2, dims=[2])

        eps = (eps_1 + eps_2) / 2.0

        sqrt_alpha_bar_t = torch.sqrt(alpha_bar_t)
        sqrt_one_minus_alpha_bar_t = torch.sqrt(1.0 - alpha_bar_t)

        x0_est = (image - sqrt_one_minus_alpha_bar_t.view(1, 1, 1, 1) * eps) / sqrt_alpha_bar_t.view(1, 1, 1, 1)

        term_1 = (torch.sqrt(alpha_bar_prev) * beta) / (1.0 - alpha_bar_t)
        term_2 = (torch.sqrt(alpha) * (1.0 - alpha_bar_prev)) / (1.0 - alpha_bar_t)

        pred_prev_image = term_1.view(1, 1, 1, 1) * x0_est + term_2.view(1, 1, 1, 1) * image
        pred_prev_image = add_variance(predicted_variance, t, pred_prev_image)

        image = pred_prev_image

        if display and ((i - i_start) % 5 == 0):
          img = (image[0].detach().cpu().permute(1, 2, 0) / 2. + 0.5).numpy()
          print(f"{timesteps[i]}")
          media.show_image(img)

      clean = image.cpu().detach().numpy()

    return clean</code></pre> -->
    
    <h6 class="subhead">Illusion 1: Castle Tower / Lighthouse</h6>
    <p class="desc"><strong>Prompts:</strong> "a castle tower drawn in thin black line art" (upright) / "a lighthouse drawn in thin black line art" (flipped)</p>
    <div class="grid cols-2">
      <figure class="tile"><img src="./media/parta/part1/1_8_castle_upright.png" alt="Castle Tower Upright"><figcaption class="cap">Castle (Upright)</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_8_lighthouse_flipped.png" alt="Lighthouse Flipped"><figcaption class="cap">Lighthouse (Flipped)</figcaption></figure>
    </div>
    
    <h6 class="subhead">Illusion 2: Forest / Mountain Lake</h6>
    <p class="desc"><strong>Prompts:</strong> "a symmetric ink drawing of a forest with mirrored trees" (upright) / "an upside-down ink drawing of mountains reflected in water" (flipped)</p>
    <div class="grid cols-2">
      <figure class="tile"><img src="./media/parta/part1/1_8_forest_upright.png" alt="Forest Upright"><figcaption class="cap">Forest (Upright)</figcaption></figure>
      <figure class="tile"><img src="./media/parta/part1/1_8_mountain_flipped.png" alt="Mountain Lake Flipped"><figcaption class="cap">Mountain (Flipped)</figcaption></figure>
    </div>

    <h4 class="subhead">1.9 Hybrid Images</h4>
    
    <p class="desc">Using Factorized Diffusion, we can create hybrid images that reveal different content depending on viewing distance. By combining frequency components from two different prompts during the denoising process, we generate images where low-frequency structure suggests one subject while high-frequency details suggest another.</p>
    
    <p class="explanation"><strong>Implementation:</strong> Similar to visual anagrams, we create a composite noise estimate, but this time by combining frequency bands:</p>
    <ul class="explanation">
      <li><strong>Generate two noise estimates:</strong> Apply CFG with both prompts $p_1$ and $p_2$ on the same noisy image $x_t$ to get $\epsilon_1$ and $\epsilon_2$</li>
      <li><strong>Extract low frequencies:</strong> Apply Gaussian blur to $\epsilon_1$ to get $f_{\text{lowpass}}(\epsilon_1)$ (coarse structure)</li>
      <li><strong>Extract high frequencies:</strong> Compute $f_{\text{highpass}}(\epsilon_2) = \epsilon_2 - f_{\text{lowpass}}(\epsilon_2)$ (fine details)</li>
      <li><strong>Combine:</strong> Sum the low frequencies from prompt 1 with high frequencies from prompt 2</li>
    </ul>
    
    <p class="explanation">The algorithm at each denoising step:</p>
    
    <p class="equation">$$\epsilon_1 = \text{CFG}(\text{UNet}(x_t, t, p_1))$$</p>
    <p class="equation">$$\epsilon_2 = \text{CFG}(\text{UNet}(x_t, t, p_2))$$</p>
    <p class="equation">$$\epsilon = f_{\text{lowpass}}(\epsilon_1) + f_{\text{highpass}}(\epsilon_2)$$</p>
    
    <p class="explanation">Using kernel size 33 and sigma 2 for Gaussian blur, this creates images where the low-frequency prompt dominates when viewed from afar, while the high-frequency prompt becomes visible up close.</p>
    
    <!-- <h6 class="subhead">Implementation</h6>
    <pre class="code"><code>def make_hybrids(image, i_start, prompt_embeds, uncond_prompt_embeds, timesteps, scale=7, display=True):
  prompt1_embeds, prompt2_embeds = prompt_embeds

  image = image.to(device).half()
  prompt1_embeds = prompt1_embeds.to(device).half()
  prompt2_embeds = prompt2_embeds.to(device).half()
  uncond_prompt_embeds = uncond_prompt_embeds.to(device).half()

  with torch.no_grad():
    for i in range(i_start, len(timesteps) - 1):
      t = timesteps[i]
      prev_t = timesteps[i + 1]

      alpha_bar_t = alphas_cumprod[t].to(device)
      alpha_bar_prev = alphas_cumprod[prev_t].to(device)
      alpha = alpha_bar_t / alpha_bar_prev
      beta = 1.0 - alpha

      image = image.to(device).half()

      model_output_1 = stage_1.unet(
          image,
          t,
          encoder_hidden_states=prompt1_embeds,
          return_dict=False
      )[0]

      uncond_output_1 = stage_1.unet(
          image,
          t,
          encoder_hidden_states=uncond_prompt_embeds,
          return_dict=False
      )[0]

      noise_1, predicted_variance = torch.split(model_output_1, image.shape[1], dim=1)
      uncond_noise_1, _ = torch.split(uncond_output_1, image.shape[1], dim=1)

      eps_1 = uncond_noise_1 + scale * (noise_1 - uncond_noise_1)

      model_output_2 = stage_1.unet(
          image,
          t,
          encoder_hidden_states=prompt2_embeds,
          return_dict=False
      )[0]

      uncond_output_2 = stage_1.unet(
          image,
          t,
          encoder_hidden_states=uncond_prompt_embeds,
          return_dict=False
      )[0]

      noise_2, _ = torch.split(model_output_2, image.shape[1], dim=1)
      uncond_noise_2, _ = torch.split(uncond_output_2, image.shape[1], dim=1)

      eps_2 = uncond_noise_2 + scale * (noise_2 - uncond_noise_2)

      eps_1_lowfreq = TF.gaussian_blur(eps_1, kernel_size=33, sigma=2.0)
      eps_2_lowfreq = TF.gaussian_blur(eps_2, kernel_size=33, sigma=2.0)
      eps_2_highfreq = eps_2 - eps_2_lowfreq

      eps = eps_1_lowfreq + eps_2_highfreq

      sqrt_alpha_bar_t = torch.sqrt(alpha_bar_t)
      sqrt_one_minus_alpha_bar_t = torch.sqrt(1.0 - alpha_bar_t)

      x0_est = (image - sqrt_one_minus_alpha_bar_t.view(1, 1, 1, 1) * eps) / sqrt_alpha_bar_t.view(1, 1, 1, 1)

      term_1 = (torch.sqrt(alpha_bar_prev) * beta) / (1.0 - alpha_bar_t)
      term_2 = (torch.sqrt(alpha) * (1.0 - alpha_bar_prev)) / (1.0 - alpha_bar_t)

      pred_prev_image = term_1.view(1, 1, 1, 1) * x0_est + term_2.view(1, 1, 1, 1) * image
      pred_prev_image = add_variance(predicted_variance, t, pred_prev_image)

      image = pred_prev_image

      if display and ((i - i_start) % 5 == 0):
        img = (image[0].detach().cpu().permute(1, 2, 0) / 2. + 0.5).numpy()
        print(f"{timesteps[i]}")
        media.show_image(img)

    clean = image.cpu().detach().numpy()

  return clean</code></pre> -->
    
    <div class="grid cols-2">
      <div>
        <h6 class="subhead">Hybrid Image 1: Moon / Gears</h6>
        <p class="desc"><strong>Prompts:</strong> "a clean watercolor illustration of the full moon" (low freq) / "complex brass clockwork machinery full of tiny gears and engravings" (high freq)</p>
        <figure class="tile"><img src="./media/parta/part1/1_9_moon_gear.png" alt="Moon / Gears Hybrid"><figcaption class="cap">Low Freq: Moon | High Freq: Gears</figcaption></figure>
      </div>
      <div>
        <h6 class="subhead">Hybrid Image 2: Sunrise Beach / Rocky Coast</h6>
        <p class="desc"><strong>Prompts:</strong> "a serene beach at sunrise, minimalistic pastel colors" (low freq) / "a rocky coastline with crashing waves, hyper-detailed realism" (high freq)</p>
        <figure class="tile"><img src="./media/parta/part1/1_9_sunset_sea.png" alt="Sunrise Beach / Rocky Coast Hybrid"><figcaption class="cap">Low Freq: Sunrise Beach | High Freq: Rocky Coast</figcaption></figure>
      </div>
    </div>

  </section>

  <!-- Part B -->
  <section class="card" id="part-b">
    <h2>Part B: Flow Matching from Scratch!</h2>
    
    <h3 class="subhead">Part 1: Training a Single-Step Denoising UNet</h3>
    <p class="desc">In this part, we build a one-step denoiser to map noisy images $z$ to clean images $x$. The goal is to train a denoiser $D_\theta$ by optimizing the L2 loss:</p>
    <p style="text-align: center; margin: 16px 0;">$\mathcal{L} = \mathbb{E}_{z,x} \|D_\theta(z) - x\|^2$</p>
    
    <h4 class="subhead">1.1 Implementing the UNet</h4>
    <p class="desc">We implement the denoiser as a UNet architecture, which consists of downsampling blocks (encoder), a bottleneck, and upsampling blocks (decoder) with skip connections. The architecture processes images through multiple resolutions to capture both local details and global structure.</p>
    <figure class="tile"><img src="./media/partb/part1/1_1_unconditional_unet.png" alt="Unconditional UNet Architecture" style="width: 100%;"><figcaption class="cap">Unconditional UNet Architecture</figcaption></figure>
    
    <h4 class="subhead">1.2 Using the UNet to Train a Denoiser</h4>
    <p class="desc">The goal is to train a denoiser $D_\theta$ that maps a noisy image $z$ to a clean image $x$. We optimize the following L2 loss:</p>
    <p style="text-align: center; margin: 16px 0;">$\mathcal{L} = \mathbb{E}_{z,x} \|D_\theta(z) - x\|^2$</p>
    <p class="desc">To generate training pairs $(z, x)$ where $x$ is a clean MNIST digit, we apply the noising process:</p>
    <p style="text-align: center; margin: 16px 0;">$z = x + \sigma \epsilon$, where $\epsilon \sim \mathcal{N}(0, I)$</p>
    <p class="desc">For each training batch, we sample random $\sigma$ values and add Gaussian noise to clean images, allowing the network to learn denoising across different noise levels. Below is a visualization of the noising process for normalized images using $\sigma = [0.0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0]$. As $\sigma$ increases, the images become progressively noisier:</p>
    <figure class="tile"><img src="./media/partb/part1/1_2_noising_process.png" alt="Noising process visualization" style="width: 100%;"><figcaption class="cap">Noising Process with Varying σ</figcaption></figure>
    
    <h5 class="subhead">1.2.1 Training</h5>
    <p class="desc">I trained a UNet denoiser (D=128) on the MNIST training set for 5 epochs with Adam optimizer (lr=1e-4, batch size=256) at noise level $\sigma = 0.5$.</p>
    
    <p class="desc"><strong>Training Loss Curve:</strong></p>
    <div style="text-align: center;">
      <figure class="tile" style="display: inline-block; width: 50%;"><img src="./media/partb/part1/1_2_1_loss_graph.png" alt="Training loss curve" style="width: 100%;"><figcaption class="cap">Training Loss Curve Over Iterations</figcaption></figure>
    </div>
    
    <p class="desc"><strong>Denoising Results:</strong> Sample results on the test set after epoch 1 and epoch 5 with $\sigma = 0.5$.</p>
    <figure class="tile"><img src="./media/partb/part1/1_2_1_epoch_1.png" alt="Denoising results after epoch 1" style="width: 100%;"><figcaption class="cap">Denoising Results - After Epoch 1 (σ = 0.5)</figcaption></figure>
    <figure class="tile"><img src="./media/partb/part1/1_2_1_epoch_5.png" alt="Denoising results after epoch 5" style="width: 100%;"><figcaption class="cap">Denoising Results - After Epoch 5 (σ = 0.5)</figcaption></figure>
    
    <h5 class="subhead">1.2.2 Out-of-Distribution Testing</h5>
    <p class="desc">The denoiser was tested on noise levels $\sigma = [0.0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0]$ to evaluate performance outside the training distribution.</p>
    <figure class="tile"><img src="./media/partb/part1/1_2_2_digit_7.png" alt="OOD denoising digit 7" style="width: 100%;"><figcaption class="cap">Out-of-Distribution Denoising Results - Digit 7</figcaption></figure>
    <figure class="tile"><img src="./media/partb/part1/1_2_2_digit_2.png" alt="OOD denoising digit 2" style="width: 100%;"><figcaption class="cap">Out-of-Distribution Denoising Results - Digit 2</figcaption></figure>
    <figure class="tile"><img src="./media/partb/part1/1_2_2_digit_1.png" alt="OOD denoising digit 1" style="width: 100%;"><figcaption class="cap">Out-of-Distribution Denoising Results - Digit 1</figcaption></figure>
    
    <h5 class="subhead">1.2.3 Denoising Pure Noise</h5>
    <p class="desc">To make denoising a generative task, I trained the denoiser on pure Gaussian noise $z \sim \mathcal{N}(0, I)$ (where $\sigma = 1.0$) to generate clean MNIST digits. The model was trained for 5 epochs with the same setup as 1.2.1.</p>
    
    <p class="desc"><strong>Training Loss Curve:</strong></p>
    <div style="text-align: center;">
      <figure class="tile" style="display: inline-block; width: 50%;"><img src="./media/partb/part1/1_2_3_loss_graph.png" alt="Training loss curve for pure noise" style="width: 100%;"><figcaption class="cap">Training Loss Curve Over Iterations (Pure Noise)</figcaption></figure>
    </div>
    
    <p class="desc"><strong>Generated Samples:</strong> Results after epoch 1 and epoch 5 when denoising pure noise.</p>
    <figure class="tile"><img src="./media/partb/part1/1_2_3_epoch_1.png" alt="Pure noise denoising after epoch 1" style="width: 100%;"><figcaption class="cap">Generated Samples - After Epoch 1</figcaption></figure>
    <figure class="tile"><img src="./media/partb/part1/1_2_3_epoch_5.png" alt="Pure noise denoising after epoch 5" style="width: 100%;"><figcaption class="cap">Generated Samples - After Epoch 5</figcaption></figure>
    
    <p class="desc"><strong>Observations:</strong> The generated outputs appear blurry and show averaged features across multiple digit classes. This occurs because the MSE loss encourages the model to predict the centroid (mean) of all training examples. Since pure noise provides no information about which specific digit to generate, the model outputs an average representation of all digits in the training set, resulting in ambiguous images rather than individual digits.</p>
    
    <h3 class="subhead">Part 2: Training a Flow Matching Model</h3>
    <p class="desc">One-step denoising does not work well for generative tasks. Instead, we use flow matching to iteratively denoise images by training a UNet to predict the flow from noise $x_0 \sim \mathcal{N}(0, I)$ to clean data $x_1$.</p>
    
    <p class="desc">Intermediate noisy samples are defined as a linear interpolation:</p>
    <p style="text-align: center; margin: 16px 0;">$x_t = (1-t)x_0 + tx_1, \quad t \in [0, 1]$</p>
    <p class="desc">For small $t$, we remain close to noise; for larger $t$, we approach the clean distribution.</p>
    
    <p class="desc">Flow represents the velocity (change in position with respect to time):</p>
    <p style="text-align: center; margin: 16px 0;">$u(x_t, t) = \frac{d}{dt} x_t = x_1 - x_0$</p>
    
    <p class="desc">We train $u_\theta$ to approximate this flow with the loss:</p>
    <p style="text-align: center; margin: 16px 0;">$\mathcal{L} = \mathbb{E}_{x_0, x_1, t} \|(x_1 - x_0) - u_\theta(x_t, t)\|^2$</p>
    
    <h4 class="subhead">2.1 Adding Time Conditioning to UNet</h4>
    <p class="desc">To condition the UNet on the scalar timestep $t$, we inject it into the network using FCBlock (fully-connected block) operations. The time signal is processed and added to feature maps at multiple points in the architecture. The model predicts the flow from noisy $x_t$ to clean $x_1$ (i.e., $x_1 - x_0$).</p>
    <figure class="tile"><img src="./media/partb/part2/2_1_conditioned_unet.png" alt="Time-Conditioned UNet Architecture" style="width: 100%;"><figcaption class="cap">Time-Conditioned UNet Architecture</figcaption></figure>
    
    <h4 class="subhead">2.2 Training the UNet</h4>
    <p class="desc">Training the time-conditioned UNet follows this process: sample a random image $x_1$ from the training set, sample a random timestep $t \in [0, 1]$, create noisy $x_t$ via linear interpolation, and train the model to predict the flow $x_1 - x_0$ at timestep $t$.</p>
    
    <p class="desc">I trained the model on MNIST for 10 epochs with the following setup: D=64, batch size=64, Adam optimizer with initial learning rate 1e-2 and exponential decay (gamma=$0.1^{1/10}$).</p>
    
    <p class="desc"><strong>Training Loss Curve:</strong></p>
    <div style="text-align: center;">
      <figure class="tile" style="display: inline-block; width: 50%;"><img src="./media/partb/part2/2_2_loss_graph.png" alt="Training loss curve" style="width: 100%;"><figcaption class="cap">Training Loss Curve Over Iterations</figcaption></figure>
    </div>
    
    <h4 class="subhead">2.3 Sampling from the UNet</h4>
    <p class="desc">To generate images, we start with pure noise $x_0 \sim \mathcal{N}(0, I)$ and iteratively apply the learned flow. At each timestep, we query the model for the predicted flow and update our sample: $x_t \leftarrow x_t + u_\theta(x_t, t)$. This process gradually transforms noise into clean digits.</p>
    
    <p class="desc"><strong>Generated Samples:</strong> Results from the time-conditioned UNet after epochs 1, 5, and 10.</p>
    <figure class="tile"><img src="./media/partb/part2/2_3_epoch_1.png" alt="Generated samples epoch 1" style="width: 100%;"><figcaption class="cap">Generated Samples - After Epoch 1</figcaption></figure>
    <figure class="tile"><img src="./media/partb/part2/2_3_epoch_5.png" alt="Generated samples epoch 5" style="width: 100%;"><figcaption class="cap">Generated Samples - After Epoch 5</figcaption></figure>
    <figure class="tile"><img src="./media/partb/part2/2_3_epoch_10.png" alt="Generated samples epoch 10" style="width: 100%;"><figcaption class="cap">Generated Samples - After Epoch 10</figcaption></figure>
    
    <h4 class="subhead">2.4 Adding Class-Conditioning to UNet</h4>
    <p class="desc">To enable controlled generation of specific digits (0-9), we add class conditioning to the UNet. The class label $c$ is encoded as a one-hot vector and injected via additional FCBlocks. To support classifier-free guidance, we implement dropout: 10% of the time ($p_{\text{uncond}} = 0.1$), the class conditioning vector $c$ is set to 0 during training. The UNet becomes $u_\theta(x_t, t, c)$, conditioned on both time $t$ and class $c$.</p>
    
    <h4 class="subhead">2.5 Training the UNet</h4>
    <p class="desc">Training the class-conditioned UNet follows the same process as the time-only version, with the addition of class conditioning vector $c$ and periodic unconditional generation (dropout probability $p_{\text{uncond}} = 0.1$).</p>
    
    <p class="desc">I trained the model on MNIST for 10 epochs with the same setup as section 2.2: D=64, batch size=64, Adam optimizer with initial learning rate 1e-2 and exponential decay.</p>
    
    <p class="desc"><strong>Training Loss Curve:</strong></p>
    <div style="text-align: center;">
      <figure class="tile" style="display: inline-block; width: 50%;"><img src="./media/partb/part2/2_5_loss_graph.png" alt="Training loss curve for class-conditioned model" style="width: 100%;"><figcaption class="cap">Training Loss Curve Over Iterations</figcaption></figure>
    </div>
    
    <h4 class="subhead">2.6 Sampling from the UNet</h4>
    <p class="desc">Sampling uses class-conditioning with classifier-free guidance ($\gamma = 5.0$). The guidance scale amplifies the conditional signal, improving generation quality. Below are 4 instances of each digit (0-9) generated after epochs 1, 5, and 10.</p>
    
    <p class="desc"><strong>Generated Samples:</strong> Class-conditioned generation shows faster convergence compared to time-only conditioning.</p>
    <figure class="tile"><img src="./media/partb/part2/2_6_epoch_1.png" alt="Class-conditioned samples epoch 1" style="width: 100%;"><figcaption class="cap">Generated Samples - After Epoch 1</figcaption></figure>
    <figure class="tile"><img src="./media/partb/part2/2_6_epoch_5.png" alt="Class-conditioned samples epoch 5" style="width: 100%;"><figcaption class="cap">Generated Samples - After Epoch 5</figcaption></figure>
    <figure class="tile"><img src="./media/partb/part2/2_6_epoch_10.png" alt="Class-conditioned samples epoch 10" style="width: 100%;"><figcaption class="cap">Generated Samples - After Epoch 10</figcaption></figure>
    
    <h5 class="subhead">Training Without Learning Rate Scheduler</h5>
    <p class="desc">To simplify training, I used a constant learning rate of 5e-3 instead of exponential decay. This lower rate avoids aggressive early updates that would require scheduling, while maintaining stable learning throughout all epochs with comparable performance.</p>
    
    <p class="desc"><strong>Generated Samples (No Scheduler):</strong></p>
    <figure class="tile"><img src="./media/partb/part2/2_6_no_scheduler_epoch_1.png" alt="No scheduler samples epoch 1" style="width: 100%;"><figcaption class="cap">Generated Samples (No Scheduler) - After Epoch 1</figcaption></figure>
    <figure class="tile"><img src="./media/partb/part2/2_6_no_scheduler_epoch_5.png" alt="No scheduler samples epoch 5" style="width: 100%;"><figcaption class="cap">Generated Samples (No Scheduler) - After Epoch 5</figcaption></figure>
    <figure class="tile"><img src="./media/partb/part2/2_6_no_scheduler_epoch_10.png" alt="No scheduler samples epoch 10" style="width: 100%;"><figcaption class="cap">Generated Samples (No Scheduler) - After Epoch 10</figcaption></figure>
  </section>

  <!-- Contact -->
  <section class="contact">
    <a href="mailto:lenci.ni@berkeley.edu?subject=CS180%20Project%205">Contact Me</a>
  </section>
</div>
</body>
</html>
